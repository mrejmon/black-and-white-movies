{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "from scipy.special import softmax\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def init():\n",
    "    return tf.keras.initializers.GlorotNormal()\n",
    "def reg():\n",
    "    # The weight decay is 1e-3 in the paper, but apparently\n",
    "    # it ought to be divided by 2 when converting Caffe model to Keras.\n",
    "    # https://bbabenko.github.io/weight-decay/\n",
    "    return tf.keras.regularizers.l2(l=5e-4)\n",
    "\n",
    "## COLORIZATION\n",
    "\n",
    "input_ = tf.keras.Input(shape=(224, 224, 1))\n",
    "conv1_1 = tf.keras.layers.Conv2D(64, (3, 3), name='conv1_1', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(input_)\n",
    "cov1_2pad = tf.keras.layers.ZeroPadding2D((1, 1))(conv1_1)\n",
    "cov1_2 = tf.keras.layers.Conv2D(64, (3, 3), name='conv1_2', activation='relu', padding='valid', strides=(2, 2),\n",
    "                                kernel_initializer=init(), kernel_regularizer = reg())(cov1_2pad)\n",
    "conv1_2norm = tf.keras.layers.BatchNormalization(name='conv1_2norm', center=False, scale=False)(cov1_2)\n",
    "\n",
    "conv2_1 = tf.keras.layers.Conv2D(128, (3, 3), name='conv2_1', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv1_2norm)\n",
    "conv2_2pad = tf.keras.layers.ZeroPadding2D((1, 1))(conv2_1)\n",
    "conv2_2 = tf.keras.layers.Conv2D(128, (3, 3), name='conv2_2', activation='relu', padding='valid', strides=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv2_2pad)\n",
    "conv2_2norm = tf.keras.layers.BatchNormalization(name='conv2_2norm', center=False, scale=False)(conv2_2)\n",
    "\n",
    "conv3_1 = tf.keras.layers.Conv2D(256, (3, 3), name='conv3_1', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv2_2norm)\n",
    "conv3_2 = tf.keras.layers.Conv2D(256, (3, 3), name='conv3_2', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv3_1)\n",
    "conv3_3pad = tf.keras.layers.ZeroPadding2D((1, 1))(conv3_2)\n",
    "conv3_3 = tf.keras.layers.Conv2D(256, (3, 3), name='conv3_3', activation='relu', padding='valid', strides=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv3_3pad)\n",
    "conv3_3norm = tf.keras.layers.BatchNormalization(name='conv3_3norm', center=False, scale=False)(conv3_3)\n",
    "\n",
    "conv4_1 = tf.keras.layers.Conv2D(512, (3, 3), name='conv4_1', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv3_3norm)\n",
    "conv4_2 = tf.keras.layers.Conv2D(512, (3, 3), name='conv4_2', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv4_1)\n",
    "conv4_3 = tf.keras.layers.Conv2D(512, (3, 3), name='conv4_3', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv4_2)\n",
    "conv4_3norm = tf.keras.layers.BatchNormalization(name='conv4_3norm', center=False, scale=False)(conv4_3)\n",
    "\n",
    "conv5_1 = tf.keras.layers.Conv2D(512, (3, 3), name='conv5_1', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv4_3norm)\n",
    "conv5_2 = tf.keras.layers.Conv2D(512, (3, 3), name='conv5_2', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv5_1)\n",
    "conv5_3 = tf.keras.layers.Conv2D(512, (3, 3), name='conv5_3', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv5_2)\n",
    "conv5_3norm = tf.keras.layers.BatchNormalization(name='conv5_3norm', center=False, scale=False)(conv5_3)\n",
    "\n",
    "conv6_1 = tf.keras.layers.Conv2D(512, (3, 3), name='conv6_1', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv5_3norm)\n",
    "conv6_2 = tf.keras.layers.Conv2D(512, (3, 3), name='conv6_2', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv6_1)\n",
    "conv6_3 = tf.keras.layers.Conv2D(512, (3, 3), name='conv6_3', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv6_2)\n",
    "conv6_3norm = tf.keras.layers.BatchNormalization(name='conv6_3norm', center=False, scale=False)(conv6_3)\n",
    "\n",
    "conv7_1 = tf.keras.layers.Conv2D(512, (3, 3), name='conv7_1', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv6_3norm)\n",
    "conv7_2 = tf.keras.layers.Conv2D(512, (3, 3), name='conv7_2', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv7_1)\n",
    "conv7_3 = tf.keras.layers.Conv2D(512, (3, 3), name='conv7_3', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv7_2)\n",
    "conv7_3norm = tf.keras.layers.BatchNormalization(name='conv7_3norm', center=False, scale=False)(conv7_3)\n",
    "\n",
    "conv8_1 = tf.keras.layers.Conv2DTranspose(256, (4, 4), name='conv8_1', activation='relu', padding='same', strides=(2,2),\n",
    "                                          kernel_initializer=init(), kernel_regularizer = reg())(conv7_3norm)\n",
    "conv8_2 = tf.keras.layers.Conv2D(256, (3, 3), name='conv8_2', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv8_1)\n",
    "conv8_3 = tf.keras.layers.Conv2D(256, (3, 3), name='conv8_3', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv8_2)\n",
    "\n",
    "conv8_313 = tf.keras.layers.Conv2D(313, (1, 1), name='conv8_313',\n",
    "                                   kernel_initializer=init(), kernel_regularizer = reg())(conv8_3)\n",
    "\n",
    "## IMAGE SEGMENTATION\n",
    "\n",
    "deconv5_1 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv5_1', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(conv5_3norm)\n",
    "deconv5_2 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv5_2', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv5_1)\n",
    "deconv5_3 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv5_3', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv5_2)\n",
    "deconv5_3norm = tf.keras.layers.BatchNormalization(name='deconv5_3norm', center=False, scale=False)(deconv5_3)\n",
    "\n",
    "deconv6_1 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv6_1', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(conv6_3norm)\n",
    "deconv6_2 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv6_2', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv6_1)\n",
    "deconv6_3 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv6_3', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv6_2)\n",
    "deconv6_3norm = tf.keras.layers.BatchNormalization(name='deconv6_3norm', center=False, scale=False)(deconv6_3)\n",
    "\n",
    "deconv7_1 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv7_1', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(conv7_3norm)\n",
    "deconv7_2 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv7_2', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv7_1)\n",
    "deconv7_3 = tf.keras.layers.Conv2DTranspose(128, (3, 3), name='deconv7_3', activation='relu', padding='same', dilation_rate=(2,2),\n",
    "                                            kernel_initializer=init(), kernel_regularizer = reg())(deconv7_2)\n",
    "deconv7_3norm = tf.keras.layers.BatchNormalization(name='deconv7_3norm', center=False, scale=False)(deconv7_3)\n",
    "\n",
    "concat = tf.keras.layers.concatenate([deconv5_3norm, deconv6_3norm, deconv7_3norm])\n",
    "\n",
    "conv9_1 = tf.keras.layers.Conv2DTranspose(256, (4, 4), name='conv9_1', activation='relu', padding='same', strides=(2,2),\n",
    "                                          kernel_initializer=init(), kernel_regularizer = reg())(concat)\n",
    "conv9_2 = tf.keras.layers.Conv2D(256, (3, 3), name='conv9_2', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv9_1)\n",
    "conv9_3 = tf.keras.layers.Conv2D(256, (3, 3), name='conv9_3', activation='relu', padding='same',\n",
    "                                 kernel_initializer=init(), kernel_regularizer = reg())(conv9_2)\n",
    "\n",
    "conv9_21 = tf.keras.layers.Conv2D(21, (1, 1), name='conv9_21',\n",
    "                                  kernel_initializer=init(), kernel_regularizer = reg())(conv9_3)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_, outputs=[conv8_313, conv9_21])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-05,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    amsgrad=False,\n",
    "    epsilon=1e-07,\n",
    ")\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "    loss={\n",
    "        'conv8_313': tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "            label_smoothing=0),\n",
    "        'conv9_21' : tf.keras.losses.CategoricalCrossentropy(\n",
    "            from_logits=True,\n",
    "            label_smoothing=0),\n",
    "    },\n",
    "    # 1:100 is the ratio given in the paper, to make the losses have similar magnitude.\n",
    "    # However from my tests ratios 1:10 or even 1:1 seemed more fitting (depending upon initialization).\n",
    "    # loss_weights = [1, 100],\n",
    "    metrics=[tf.keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "tf.keras.utils.plot_model(model, 'graph.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes colorization branch according to the weights given by Zhang et al.\n",
    "# kmeans_init = weights before training, initialized with k-means.\n",
    "# pretrained = weights after training.\n",
    "# model.load_weights('models/zhang_kmeans_init.h5', by_name=True)\n",
    "# model.load_weights('models/zhang_pretrained.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing & Data Reading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GLOBAL VARIABLES\n",
    "\n",
    "gamut_vals_p = 'resources/gamut_vals.npy'\n",
    "color_weights_p = 'resources/color_weights.npy'\n",
    "train_txt_p = 'resources/train.txt'\n",
    "val_txt_p = 'resources/val.txt'\n",
    "test_txt_p = 'resources/val.txt'\n",
    "img_p = 'data/VOCdevkit/VOC2012/JPEGImages/'\n",
    "img_ext = '.jpg'\n",
    "segm_p = 'data/VOCdevkit/VOC2012/SegmentationClass/'\n",
    "segm_ext = '.png'\n",
    "model_name = 'MVI'\n",
    "model_p = 'models/' + model_name\n",
    "tensorboard_p = 'tensorboard/' + model_name\n",
    "\n",
    "# Used when computing \"expected value\" of the resulting color distribution.\n",
    "gamut_vals = np.squeeze(np.load(gamut_vals_p))\n",
    "# Used to rebalance colors based on their rarity.\n",
    "color_weights = np.load(color_weights_p)\n",
    "# Used to find closest colors when smoothing labels (soft encoding).\n",
    "neighbors = NearestNeighbors(algorithm='auto', metric='minkowski', p=2).fit(gamut_vals)\n",
    "\n",
    "## PREPROCESSING\n",
    "\n",
    "def preprocess_x(imgs):\n",
    "    imgs = [preprocess_x_(img) for img in imgs]\n",
    "    imgs = np.stack(imgs)\n",
    "    return imgs\n",
    "\n",
    "def preprocess_x_(img):\n",
    "    img = ski.transform.resize(img, (224, 224, 3))\n",
    "    img = ski.color.rgb2lab(img)\n",
    "    img = img[:, :, :1]\n",
    "    # Zhang et al. subtract 50 from each input image for \"mean-centering\",\n",
    "    # even though they don't mention it directly in the paper.\n",
    "    # Possibly it is the mean of the whole ImageNet dataset (in 0-255 values) ?\n",
    "    # What is interesting is that the ski.transform.resize() method\n",
    "    # transforms the input range from 0-255 to 0-1, thus all the values\n",
    "    # stay around -50. The pretrained model does not perform well without it though.\n",
    "    img = img - 50\n",
    "    return img\n",
    "\n",
    "def preprocess_img_y(imgs):\n",
    "    imgs = [preprocess_img_y_(img) for img in imgs]\n",
    "    imgs = np.stack(imgs)\n",
    "    return imgs\n",
    "\n",
    "def preprocess_img_y_(img):\n",
    "    img = ski.transform.resize(img, (56, 56, 3))\n",
    "    img = ski.color.rgb2lab(img)\n",
    "    img = img[:, :, 1:]\n",
    "    img = soft_encode(img)\n",
    "    # The classes are rebalanced here in preprocessing and\n",
    "    # not during loss calculation,, as to the best of my knowledge\n",
    "    # thanks to the distributive law it should not matter,\n",
    "    # and it enables me to not have to use custom losses.\n",
    "    img = rebalance_classes(img)\n",
    "    return img\n",
    "\n",
    "def soft_encode(img):\n",
    "    img = np.reshape(img, (3136, 2))\n",
    "    # In the paper n_neighbors is equal to 5, but I found in a GitHub issue it should be 10.\n",
    "    # https://github.com/richzhang/colorization/issues/59\n",
    "    dist, ind = neighbors.kneighbors(img, n_neighbors=10)\n",
    "    sigma = 5\n",
    "    # This line \"weights the neighbors proportionally to their distance from\n",
    "    # the ground truth usinga Gaussian kernel with sigma = 5.\"\n",
    "    dist_norm = softmax(- ((dist ** 2) / (2 * (sigma ** 2))), axis=-1)\n",
    "    img = np.zeros((3136, 313))\n",
    "    tmp = np.arange(3136)[:, np.newaxis]\n",
    "    img[tmp, ind] = dist_norm\n",
    "    img = np.reshape(img, (56, 56, 313))\n",
    "    return img\n",
    "\n",
    "def rebalance_classes(img):\n",
    "    ind = np.argmax(img, axis=-1)\n",
    "    img_weights = color_weights[ind]\n",
    "    img_weights = img_weights[:, :, np.newaxis]\n",
    "    img = img * img_weights\n",
    "    return img\n",
    "\n",
    "def preprocess_segm_y(masks):\n",
    "    masks = [preprocess_segm_y_(mask) for mask in masks]\n",
    "    masks = np.stack(masks)\n",
    "    return masks\n",
    "\n",
    "def preprocess_segm_y_(mask):\n",
    "    # These arguments are important to keep the proper class values. Same during postprocessing.\n",
    "    mask = ski.transform.resize(mask, (56, 56), order=0, preserve_range=True, anti_aliasing=False).astype('uint8')\n",
    "    mask = tf.one_hot(mask, 21).numpy()\n",
    "    return mask\n",
    "\n",
    "## POSTPROCESSING\n",
    "\n",
    "def postprocess_color(y_test, test_imgs):\n",
    "    return [postprocess_color_(a, b) for a, b in zip(y_test, test_imgs)]\n",
    "    \n",
    "def postprocess_color_(y_test, test_img):\n",
    "    # Turn logits into probabilities, with T being the temperature parameter described in the paper.\n",
    "    T = 0.38\n",
    "    y_test = softmax(y_test / T, axis=-1)\n",
    "    # Turn probabilities into discrete values by computing their \"expected value\".\n",
    "    ab_vals = y_test @ gamut_vals\n",
    "    ab_vals = ski.transform.resize(ab_vals, (test_img.shape[0], test_img.shape[1], 2))\n",
    "    out_img = ski.color.rgb2lab(test_img)\n",
    "    out_img = out_img[:, :, :1]\n",
    "    out_img = np.concatenate((out_img, ab_vals), axis=-1)\n",
    "    out_img = ski.color.lab2rgb(out_img)\n",
    "    out_img = (255 * np.clip(out_img, 0, 1)).astype('uint8')\n",
    "    return out_img\n",
    "\n",
    "def postprocess_segm(segm_out, test_imgs):\n",
    "    return [postprocess_segm_(a, b) for a, b in zip(segm_out, test_imgs)]\n",
    "    \n",
    "def postprocess_segm_(segm_out, test_img):\n",
    "    mask = np.argmax(segm_out, axis=-1)\n",
    "    mask = ski.transform.resize(mask, (test_img.shape[0], test_img.shape[1]), order=0, preserve_range=True, anti_aliasing=False).astype('uint8')\n",
    "    return mask\n",
    "\n",
    "## DATA READING\n",
    "\n",
    "class MyDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, usage, batch_size):\n",
    "        if usage == 'train':\n",
    "            filename = train_txt_p\n",
    "        elif usage == 'val':\n",
    "            filename = val_txt_p\n",
    "        else:\n",
    "            ...\n",
    "            \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with open(filename, 'r') as f:\n",
    "            self.names = f.read().splitlines()\n",
    "            np.random.shuffle(self.names)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.names) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imgs = [ski.io.imread(img_p + name + img_ext) for name in self.names[index * self.batch_size : (index + 1) * self.batch_size]]\n",
    "        masks = [ski.io.imread(segm_p + name + segm_ext, pilmode='P') for name in self.names[index * self.batch_size : (index + 1) * self.batch_size]]\n",
    "        # Half the time mirrors a batch.\n",
    "        if np.random.random() < 1/2: \n",
    "            imgs = [np.fliplr(img) for img in imgs]\n",
    "            masks = [np.fliplr(mask) for mask in masks]\n",
    "        return preprocess_x(imgs), {'conv8_313' : preprocess_img_y(imgs), 'conv9_21' : preprocess_segm_y(masks)}\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.names)\n",
    "\n",
    "# Creates the color map associated with the PASCAL VOC2012 dataset - code is copied from:\n",
    "# https://gist.github.com/wllhf/a4533e0adebe57e3ed06d4b50c8419ae\n",
    "def color_map(N=256, normalized=False):\n",
    "    def bitget(byteval, idx):\n",
    "        return ((byteval & (1 << idx)) != 0)\n",
    "\n",
    "    dtype = 'float32' if normalized else 'uint8'\n",
    "    cmap = np.zeros((N, 3), dtype=dtype)\n",
    "    for i in range(N):\n",
    "        r = g = b = 0\n",
    "        c = i\n",
    "        for j in range(8):\n",
    "            r = r | (bitget(c, 0) << 7-j)\n",
    "            g = g | (bitget(c, 1) << 7-j)\n",
    "            b = b | (bitget(c, 2) << 7-j)\n",
    "            c = c >> 3\n",
    "\n",
    "        cmap[i] = np.array([r, g, b])\n",
    "\n",
    "    cmap = cmap/255 if normalized else cmap\n",
    "    return cmap\n",
    "        \n",
    "## CALLBACKS\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_p + '_last.h5',\n",
    "        save_freq='epoch',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_p + '_best.h5',\n",
    "        save_freq='epoch',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        factor=1/3,\n",
    "        patience=15,\n",
    "        cooldown=5,\n",
    "        min_delta=0.,\n",
    "        min_lr=1e-06\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=tensorboard_p,\n",
    "        histogram_freq=0,\n",
    "        write_graph=False,\n",
    "        write_images=False,\n",
    "        update_freq='epoch',\n",
    "        profile_batch=0,\n",
    "        embeddings_freq=0,\n",
    "        embeddings_metadata=None,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    MyDataGen('train', 40),\n",
    "    validation_data=MyDataGen('val', 40),\n",
    "    epochs=1000,\n",
    "    verbose=2,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "names = glob.glob('data/test/*')\n",
    "imgs = [ski.io.imread(name) for name in names]\n",
    "x = preprocess_x(imgs)\n",
    "\n",
    "color_out, segm_out = model.predict(x)\n",
    "\n",
    "imgs_out, masks_out = postprocess_color(color_out, imgs), postprocess_segm(segm_out, imgs)\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('pascalVOC2012', color_map(256, True), N=256)\n",
    "for a, b, c in zip(imgs, imgs_out, masks_out):\n",
    "    bw = ski.color.rgb2gray(a)\n",
    "    _, axes = plt.subplots(1, 4, figsize=(16, 16))\n",
    "    axes[0].imshow(bw, cmap='gray')\n",
    "    axes[1].imshow(a)\n",
    "    axes[2].imshow(b)\n",
    "    axes[3].imshow(c, cmap=cmap)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
